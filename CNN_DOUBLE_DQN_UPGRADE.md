# ğŸš€ CNN + Double DQN - Upgrade Complet !

## âœ… Modifications AppliquÃ©es

Votre fichier `dqn_agent.py` a Ã©tÃ© mis Ã  jour avec deux amÃ©liorations majeures :

### 1. Architecture CNN (ConvDQN)

**Avant** : MLP linÃ©aire (perd l'info spatiale)
```python
Input (16) â†’ Dense(128) â†’ Dense(128) â†’ Output(4)
```

**Maintenant** : CNN (capture les patterns spatiaux)
```python
Input (16) â†’ Reshape(1,4,4) â†’
Conv2D(64, kernel=2Ã—2) â†’ ReLU â†’ [3Ã—3]
Conv2D(128, kernel=2Ã—2) â†’ ReLU â†’ [2Ã—2]
Flatten(512) â†’
Dense(512) â†’ ReLU â†’
Dense(256) â†’ ReLU â†’  
Output(4)
```

**Avantages** :
- âœ… DÃ©tecte les **tuiles adjacentes mergables**
- âœ… Capture la **structure du plateau**
- âœ… ReconnaÃ®t les **patterns de coins**
- âœ… Meilleure **gÃ©nÃ©ralisation**

### 2. Double DQN

**Avant (DQN Standard)** :
```python
# Utilise target_net pour TOUT
next_value = target_net(next_state).max()
Q_target = reward + gamma * next_value
```

**ProblÃ¨me** : **Surestimation** des Q-values (biais positif)

**Maintenant (Double DQN)** :
```python
# SÃ©pare sÃ©lection et Ã©valuation
best_action = policy_net(next_state).argmax()  # SELECT avec policy
next_value = target_net(next_state)[best_action]  # EVALUATE avec target
Q_target = reward + gamma * next_value
```

**Avantages** :
- âœ… RÃ©duit la **surestimation** des Q-values
- âœ… **Apprentissage plus stable**
- âœ… **Convergence plus rapide**
- âœ… **Meilleures performances finales**

## ğŸ“Š Impact Attendu

### Performance AmÃ©liorÃ©e

| MÃ©trique | MLP Standard | CNN + Double DQN |
|----------|-------------|------------------|
| **Score Moyen** | 1200-3000 | **4000-8000** ğŸš€ |
| **Tuile Max** | 128-256 | **512-1024** ğŸ† |
| **Episodes nÃ©cessaires** | 10000-20000 | **5000-10000** âš¡ |
| **StabilitÃ©** | Moyenne | **Ã‰levÃ©e** âœ… |

### Nombre de ParamÃ¨tres

**MLP** (ancien) : ~34k paramÃ¨tres  
**ConvDQN** (nouveau) : **~329k paramÃ¨tres** (10x plus de capacitÃ© !)

## ğŸ® Utilisation

### Par DÃ©faut (CNN activÃ©)

```python
from dqn_agent import DQNAgent

# Utilise automatiquement CNN + Double DQN
agent = DQNAgent()
```

### Mode Legacy (MLP)

```python
# Si vous voulez l'ancien MLP
agent = DQNAgent(use_cnn=False, hidden_sizes=[128, 128])
```

### Avec vos HyperparamÃ¨tres Optuna

```python
import json

# Charger config Optuna
with open('optuna_results/best_hyperparameters.json') as f:
    params = json.load(f)

# CrÃ©er agent avec CNN
agent = DQNAgent(
    lr=params['lr'],
    gamma=params['gamma'],
    memory_capacity=params['memory_capacity'],
    use_cnn=True  # Force CNN (recommandÃ© !)
)
```

## âš ï¸ Notes Importantes

### CompatibilitÃ© Checkpoints

**Les anciens modÃ¨les MLP ne sont PAS compatibles avec CNN** (architectures diffÃ©rentes).

Si vous avez des checkpoints existants :
- Option 1 : Utiliser `use_cnn=False` pour charger anciens modÃ¨les
- Option 2 : Recommencer l'entraÃ®nement avec CNN (recommandÃ©)

### Vitesse d'EntraÃ®nement

**CNN est lÃ©gÃ¨rement plus lent** (~10-20%) que MLP MAIS :
- âœ… Converge **2-3x plus vite** (moins d'Ã©pisodes)
- âœ… Atteint de **bien meilleurs scores**
- âœ… **GPU accÃ©lÃ¨re beaucoup** le CNN

Sur **RTX 4070** : DiffÃ©rence nÃ©gligeable !

## ğŸš€ Recommandations

### Pour DÃ©marrer un Nouvel EntraÃ®nement

```bash
# Avec Optuna (pour trouver les meilleurs params CNN)
.\.venv_gpu\Scripts\python optimize_dqn.py --n-trials 50

# Ou entraÃ®nement direct avec CNN
.\.venv_gpu\Scripts\python train_dqn_fast.py 5000
```

### Dans l'UI

Le nouveau CNN est utilisÃ© automatiquement dans :
- âœ… `ui/optuna_train_screen.py` (si Optuna utilisÃ©)
- âš ï¸ Ancien `ui/dqn_train_screen.py` (peut nÃ©cessiter mise Ã  jour)

## ğŸ“ˆ RÃ©sultats Attendus

### AprÃ¨s 5000 Ã‰pisodes (CNN + Double DQN + GPU)

**Sans optimisation** :
- Score : **5000-10000**
- Tuile max : **512-1024**
- Temps : ~1-2h sur RTX 4070

**Avec Optuna** :
- Score : **8000-16000**
- Tuile max : **1024-2048**
- Temps : 4-6h (optim) + 1-2h (train)

### Comparaison Globale

| IA | Architecture | Score | EntraÃ®nement |
|----|-------------|-------|--------------|
| Heuristic | RÃ¨gles codÃ©es | 2000-4000 | Aucun |
| Expectimax | Recherche | 4000-8000 | Aucun |
| DQN (MLP) | Linear | 1200-3000 | 10000 ep |
| **DQN (CNN)** | **Convolutions** | **5000-10000** | **5000 ep** |
| **DQN (CNN+Optuna)** | **OptimisÃ©** | **8000-16000** | **5000 ep** |

## ğŸ”§ DÃ©pannage

### "RuntimeError: mat1 and mat2 shapes cannot be multiplied"

â†’ ProblÃ¨me de dimensions. Le CNN reshape automatiquement de [16] Ã  [1,4,4].

### "CUDA out of memory"

â†’ Le CNN utilise plus de VRAM. Solutions :
- RÃ©duire `batch_size` : 256 â†’ 128
- Fermer autres applications GPU

### Performances pas meilleures

â†’ Causes possibles :
1. Pas assez d'Ã©pisodes (minimum 3000-5000)
2. Learning rate trop faible/Ã©levÃ©
3. Reward shaping inadaptÃ©

## âœ… VÃ©rification

Pour confirmer que le CNN fonctionne :

```bash
.\.venv_gpu\Scripts\python -c "from dqn_agent import DQNAgent; agent = DQNAgent(); print('Architecture:', agent.policy_net.__class__.__name__)"
```

**RÃ©sultat attendu** : `Architecture: ConvDQN`

---

## ğŸ¯ Prochaines Ã‰tapes

1. âœ… **Test l'upgrade** : `.\.venv_gpu\Scripts\python train_dqn_fast.py 1000`
2. âœ… **Comparer** : Anciens checkpoints MLP vs nouveaux CNN
3. âœ… **Optimiser** : Relancer Optuna avec CNN activÃ©
4. âœ… **Profiter** : Des scores 2-3x meilleurs ! ğŸš€

**Le CNN + Double DQN est prÃªt. Lancez l'entraÃ®nement et observez la diffÃ©rence !** ğŸ®âš¡
